{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# MongoDB connection URI\n",
    "uri = \"mongodb://admin:b3BYFU0kJZpGNK6Dt42V@node1-bffd0a8e5302ff2a.database.cloud.ovh.net,node2-bffd0a8e5302ff2a.database.cloud.ovh.net,node3-bffd0a8e5302ff2a.database.cloud.ovh.net/admin?replicaSet=replicaset&tls=true\"\n",
    "\n",
    "\n",
    "try :\n",
    "    # Connect to the MongoDB cluster\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    db = client[\"ddos_detection\"]\n",
    "    collection = db[\"traffic_features\"]\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the client connection\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function to preprocess the JSON data\n",
    "def preprocess_json_data(collection):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Define feature names for interpretability\n",
    "    feature_names = [\n",
    "        'tcp_syn_flag_ratio',\n",
    "        'udp_port_entropy',\n",
    "        'avg_pkt_size',\n",
    "        'flow_density',\n",
    "        'ip_entropy'\n",
    "    ]\n",
    "\n",
    "    # Iterate through the MongoDB collection\n",
    "    for entry in collection.find():\n",
    "        # Extract features\n",
    "        tcp_syn_flag_ratio = (entry.get('tcp_syn_fwd_count', 0) + entry.get('tcp_syn_bwd_count', 0)) / (\n",
    "                entry.get('fwd_packet_count', 1) + entry.get('bwd_packet_count', 1))\n",
    "        udp_port_entropy = entry.get('unique_udp_source_ports', 0) * entry.get('unique_udp_dest_ports', 0)\n",
    "        avg_pkt_size = (entry.get('avg_fwd_pkt_size', 0) + entry.get('avg_bwd_pkt_size', 0)) / 2\n",
    "        flow_density = entry.get('flow_packets_per_sec', 0) / entry.get('flow_bytes_per_sec', 1)\n",
    "        ip_entropy = entry.get('source_ip_entropy', 0) + entry.get('dest_port_entropy', 0)\n",
    "\n",
    "        # Append features to X\n",
    "        X.append([\n",
    "            tcp_syn_flag_ratio,\n",
    "            udp_port_entropy,\n",
    "            avg_pkt_size,\n",
    "            flow_density,\n",
    "            ip_entropy\n",
    "        ])\n",
    "\n",
    "        # Append target variable (label)\n",
    "        label = entry.get('label')\n",
    "        if label == 'BENIGN':\n",
    "            y.append(0)\n",
    "        elif label == 'UDP_FLOOD':\n",
    "            y.append(1)\n",
    "        elif label == 'TCP_SYN_FLOOD':\n",
    "            y.append(2)\n",
    "        else:\n",
    "            y.append(-1)  # Adjust this as necessary for unknown labels\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Handle missing values (replace with 0)\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "    # Return the scaled features, labels, and feature names\n",
    "    return imputer, scaler, X_scaled, y, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd  # Only for feature importance DataFrame, not for `X` processing.\n",
    "import joblib  # Import joblib to save the model\n",
    "\n",
    "try:\n",
    "    # Connect to the MongoDB cluster\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    \n",
    "    # Preprocess data and get feature names\n",
    "    imputer, scaler, X_scaled, y, feature_names = preprocess_json_data(collection)\n",
    "\n",
    "    # Save the scaler to a .pkl file\n",
    "    scaler_filename = 'scaler.pkl'\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Standard Scaler saved to {scaler_filename}\")\n",
    "\n",
    "    # Save the imputer to a .pkl file\n",
    "    imputer_filename = 'imputer.pkl'\n",
    "    joblib.dump(imputer, imputer_filename)\n",
    "    print(f\"Imputer saved to {imputer_filename}\")\n",
    "\n",
    "    # Train-test split (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    tree_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "    tree_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = tree_model.predict(X_test)\n",
    "\n",
    "    # Evaluation: Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Feature importances\n",
    "    feature_importances = tree_model.feature_importances_\n",
    "\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Print the feature importance\n",
    "    print(feature_importance_df)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance in Random Forest Model')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model to a .pkl file\n",
    "    model_filename = 'decision_tree_model.pkl'\n",
    "    joblib.dump(tree_model, model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the client connection\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the saved imputer and scaler\n",
    "scaler = joblib.load('scaler.pkl')  # Load the saved scaler\n",
    "imputer = joblib.load('imputer.pkl')  # Load the saved imputer\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model_path='decision_tree_model.pkl'):\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "# Function to predict a new sample\n",
    "def predict_sample(model, scaler, imputer, sample_data):\n",
    "    # Impute missing values using the saved imputer (transform the data)\n",
    "    sample_data_imputed = imputer.transform([sample_data])  # Shape must be (1, n_features)\n",
    "    \n",
    "    # Debug: Print imputed sample data to check\n",
    "    print(\"Imputed sample data:\", sample_data_imputed)\n",
    "    \n",
    "    # Standardize the new sample using the saved scaler (transform the data)\n",
    "    sample_data_scaled = scaler.transform(sample_data_imputed)  # Shape must be (1, n_features)\n",
    "    \n",
    "    # Debug: Print scaled sample data to check\n",
    "    print(\"Scaled sample data:\", sample_data_scaled)\n",
    "    \n",
    "    # Make prediction using the loaded model\n",
    "    prediction = model.predict(sample_data_scaled)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example new sample\n",
    "new_sample = [-0.78267391, -0.14192909, -0.11761717, -0.41677815,  2.21815411]\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path='decision_tree_model.pkl')\n",
    "\n",
    "# Get prediction for the new sample\n",
    "prediction = predict_sample(model, scaler, imputer, new_sample)\n",
    "\n",
    "if len(prediction) > 0:\n",
    "    prediction = prediction[0]\n",
    "\n",
    "print(\"Prediction for new sample:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
